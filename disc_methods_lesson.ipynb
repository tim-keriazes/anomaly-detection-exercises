{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probabilistic Methods (Discrete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "import env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Anomaly Detection:** Identification of items, events or observations which is significantly different from the remaining data.\n",
    "\n",
    "- Non-parametric approach\n",
    "- Frequency or counting based\n",
    "    - How many time a value of variable (e.g. ip address) shows up\n",
    "    - More frequent - less likely to be an anomaly\n",
    "    - less frequent - more likely to be an anomaly\n",
    "    - Calculate probability \n",
    "\n",
    "    \n",
    "- Conditional probability \n",
    "    $$ {P(A|B) = }\\frac{\\text{P(A U B)}}{\\text{P(B)}} $$\n",
    "    \n",
    "    \n",
    " Examples: \n",
    "- How many times we see an ip address in the dataset (count)\n",
    "- What is probability of ip address showing up in the dataset (ip count / total observations)\n",
    "- Conditional probability. Given an ip address, what is prob of a particular status(e.g authentication failure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrangle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data comes from the webserver logs of the API that we used in the timeseries module. Each row is one request to the server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = f'mysql+pymysql://{env.user}:{env.password}@{env.host}/logs'\n",
    "df = pd.read_sql('SELECT * FROM api_access', url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['entry'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['entry'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['entry'][0].split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parts = df['entry'][0].split()\n",
    "output = {}\n",
    "\n",
    "output['ip'] = parts[0]\n",
    "output['timestamp'] = parts[3][1:].replace(':', ' ', 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# function to deal with parsing one entry in our log data\n",
    "def parse_log_entry(entry):\n",
    "    parts = entry.split()\n",
    "    output = {}\n",
    "    output['ip'] = parts[0]\n",
    "    output['timestamp'] = parts[3][1:].replace(':', ' ', 1)\n",
    "    output['request_method'] = parts[5][1:]\n",
    "    output['request_path'] = parts[6]\n",
    "    output['http_version'] = parts[7][:-1]\n",
    "    output['status_code'] = parts[8]\n",
    "    output['size'] = int(parts[9])\n",
    "    output['user_agent'] = ' '.join(parts[11:]).replace('\"', '')\n",
    "    return pd.Series(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.entry.apply(parse_log_entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Synthetic data\n",
    "new = pd.DataFrame([\n",
    "    [\"95.31.18.119\", \"21/Apr/2019 10:02:41\", \"GET\", \"/api/v1/items/\", \"HTTP/1.1\", '200', 1153005, \"python-requests/2.21.0\"],\n",
    "    [\"95.31.16.121\", \"17/Apr/2019 19:36:41\", \"GET\", \"/api/v1/sales?page=79/\", \"HTTP/1.1\", '301', 1005, \"python-requests/2.21.0\"],\n",
    "    [\"97.105.15.120\", \"18/Apr/2019 19:42:41\", \"GET\", \"/api/v1/sales?page=79/\", \"HTTP/1.1\", '301', 2560, \"python-requests/2.21.0\"],\n",
    "    [\"97.105.19.58\", \"19/Apr/2019 19:42:41\", \"GET\", \"/api/v1/sales?page=79/\", \"HTTP/1.1\", '200', 2056327, \"python-requests/2.21.0\"],\n",
    "], columns=df.columns)\n",
    "\n",
    "df = df.append(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['size_mb'] = df['size'] / 1024 / 1024\n",
    "df.timestamp = pd.to_datetime(df.timestamp)\n",
    "df = df.set_index('timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detecting Anomalies in Discrete Variables\n",
    "\n",
    "- **count**: the number of times each unique value appears in the dataset\n",
    "- **frequencies**: the number of times each unique value appears in the dataset as a percentage of the total; the count divided by the total number of observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.ip.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ip_df = pd.DataFrame(df.ip.value_counts(dropna=False)).reset_index().\\\n",
    "                rename(columns={'index': 'ip', 'ip': 'count'})\n",
    "ip_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate probabity for each ip \n",
    "\n",
    "# ip_prob = count for each ip / total count in the dataframe\n",
    "\n",
    "ip_df2 = pd.DataFrame((df.ip.value_counts(dropna=False))/df.ip.count()).reset_index().\\\n",
    "                rename(columns={'index': 'ip', 'ip': 'proba'})\n",
    "ip_df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the two data frames create above into a single one:\n",
    "ip_df = ip_df.merge(ip_df2, on='ip')\n",
    "ip_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here we can find how many unique ip addresses there are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip_df.set_index('ip')['count'].sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip_df.set_index('ip')['count'].sort_values().tail(5).plot.barh(figsize=(5,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip_df.set_index('ip')['count'].sort_values().plot.barh(figsize=(5,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional Probabilities\n",
    "\n",
    "- What is probability of a certain status code given an IP address?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HTTP Status Codes\n",
    "\n",
    "- 200: ok\n",
    "- 3xx: redirects\n",
    "- 4xx: client level errors -- the requester did something wrong\n",
    "- 5xx: server level errors -- the server did something wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prob(A|B) = prob(A & B)/prob(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pab = df.groupby(['ip', 'status_code']).size()/len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pb = df.groupby('ip').size()/len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pab / Pb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a particular ip, what is probability of a certain status code\n",
    "status_given_ip = (\n",
    "    df.groupby('ip')\\\n",
    "    .status_code.value_counts(normalize=True)\\\n",
    "    .rename('proba_status_given_ip')\\\n",
    "    .reset_index())\n",
    "status_given_ip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status_given_ip[status_given_ip.proba_status_given_ip < 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.ip == '72.181.113.170'].sort_values(by='status_code', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Cases where the probability is < 100%\n",
    "* Status codes other than 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status_given_ip[status_given_ip.status_code != 200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How do we use this if we are looking to apply assumptions on unseen data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a training set\n",
    "train = df.loc['2019-04-16 19:34:42':'2019-04-17 12:55:14']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.size / df.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add these probabilities to original events to detect anomalous events:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status_given_ip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at any unseen data in df versus assumptions made on train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index().merge(status_given_ip, on=['ip', 'status_code'], how='left').fillna(value=0).set_index('timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.proba_status_given_ip < 0.15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
